{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red20\green20\blue20;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c10196\c10196\c10196;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf2 #SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-1\
Last lec\
most core features we have covered\
Nvidia has been adding higher level features but roughly seem to be largely built out the same core features\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-2\
HW we know that bastic HW org\
   differs across cc version of your device \'97 lookup\
SW kernels, warps, blocks and grids\
GPU memories:  Global, constants,  Shared memory, registers\
kernel invocation, device syncronization\
partitioning your work and implications on occupancy\
last lect we started talking about syncronizing threads in a block\'85. today more on sync\
block execution order is arbitary and \'93stikcks to a SM\'94 \
\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-3\
nice property is that it ensure both\
1. all threads are at the same point. \'97 the obvious and easy to understand\
2. all writes are visible \'97 includes a memory fence!  more on this\
\
\
so how can we sync across threads of all blocks?\
\
break our work in two  kernels \
kernel execution in a stream is serial.\
kernel 2 waits for one\
 \
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-4\
Some other fancy syncs that inlude computation\
but not discussed much  \'97 seems people don\'92t really use them\
\
Except the last one \
\
returns the value to all thread\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-5\
before we go on it is worth noting there are now some advanced features\
that extend what we have learnt and perhaps even break it\
\
CGs:  care group threads both with and across block allowing you to barrier in finer granuality\
as a matter of fact even bigger\'85. claim is that hardware is acclerates these based on HPC\
I assume some form of combining tree to get logrithm messages exchanged\
\
Graphs:\
Allow one to combine CUDA commands from one or more streams into a more complex unit\
that can be submitted once and rerun without host interactions.\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-6\
while on the topic of syncronization \
lets look at the common mechanisms and ideas around mutual exclusion\
eg. lock\
but remeber we want to avoid this as much as possible\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-7\
Quick basic reminder of why we need mutual exclusion\
\
Shared counter that we want to increment\
\
4 threads of a warp \
\
assume 42 \
and one and then 43\
all write to count \
\
This is not likely what we want.\
\
we want a critical section around ++\
one at a time\
\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-8\
Nvidia gpu provide CAS which is very common atomic primitive included on most CPUs\
\
go over the arguments \
processor will serials \
\
Walk through the  logic\
\
if you are not familar this is very standard look it up in ans OS\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-9\
Example from NVIDIA is slightly fancier than a typical example, as it adds exponential back off\
\
walk though\
\
Note busy wait, and that without the nanosleep, we would be hammering the memory system.\
\
But the point is that only one thread will hold the mutex; contenders will spin and sleep\
Note might reduce load on memory, but longer latency for waiters to detect release.\
\
Keep critical sections very small!!!\
We want to avoid trying to use it \'97 use sparingly \
\
But there is also a more fundamental problem \
\
\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-10\
These systems have a relaxed memory model \
\
Just we because we released the lock does not mean our memory writes (side effects) have made it\
\
So other threads might not see what we did!!1\
\
will talk about fences in a couple of sides\
\
Discuss warp SIMT implications old versus new hardware \
Indedependent thread scheduling make it ok post volta\
pre-volta deadlock\
\
But regardless performance would istill be very painful so warp syncnonization sould be carefull\
written \
\
eg only have one thread of each work be a master with respect to what you want\
threadid %32 do lock or work\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-11\
to make our life easier NVIDIA provides a rich set of atomics that avoid the need for a real lock built\
out of CAS\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-12\
Ok lets be a little more precise \'97 only a little\
\
Modern world of multi-cores programmers / languages have started to ackwledge the difficults and codify \
\
what one can expect from a system\'92s memory behaviour \
\
GPU \'97 relaxed memory \'97 out of order writes\
\
But the other aspect that NVIDIA has started to codify is the notion of scope\
that is to say what set of thread \'97 given hetrogenous arhitecture\
that  are we trying to control\
\
Can you guess why?\
\
waiting for all writes across the entire system might cost a lot more than waiting for writes on my SM\
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-13\
even more preciesely \
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-14\
To be make sure we are being concrete incase you have never thought about the implications of out of order exectuion / weake memory ordering\
\
consider \
key to not this is not because two threads are racing to write to the variables\
four out comes\
\
X = 20 comese after X=10 on a single thread \'97 program order of referred to \
and yet we can observe the values that might imply racing with other threads or \
just a broken world\
\
note how scope comes in to play\
\
threadfence_block \'97 writes visible by all threads on the SM\
vs \
threadfence \'97 writes visible by all threads across all SMs\
which might be more expensive?\
so while threadfence would be always safe to use being more preciese if all you need is to coordinate within your block then threadfence_block would be better\
\
\
 \
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-15\
Remember if we are trying to ensure memory behaviour we often need to tell the compiler to not optimize our access \
\
tradiional to use __volatilie__\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-16\
Before we move on to talking about exploting asyncrony\
\
I want to let you know that there are a bunch very interesting preimitive for doing syncronous operations \
within a warp\
\
can exchange data and compute lane wide global values\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
Not going into details \'97 see manual\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-17\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
my intuituion is that one can creatively use these to squeeze out more performance (but buyer be ware \'97 sleep is also a nice thing)\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-18\
On to our final topic\
\
Classic system optimization\
\
pipeline\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-19\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
My main goal is to point out these features so you are aware but not to cover them in depth\
\
you can read the manuals and look at examples\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-20\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
programmig interface object  oriented\
\
create object\
auto block = cooperateive_groups::this_thread_block();\
\
to wait\
block.sync()\

\f1 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
TMA is another new async memory feature that has been added.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-21\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
A big goal has been to evolve the model to avoid the amount of time we \
are waiting for I/O but getting overlaps \
also get compute overlap \
\
Key mechanism is streams\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-22\
Remember DMA engine on the GPU do the real memory movement\
and we have lots of compute resources\
the game is to get streams of work going that over lap this sthings\
\
commands in a stream are serial\
however across stremas they can be concurrent\
\
create a stream get a handle and add work to it\
\
need the resouce and to break up your work appropriately \
for this to be worth while\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-23\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-24\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-25\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-26\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-27\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-28\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-29\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-30\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-31\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-32\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-33\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-34\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-35\
\
#SLIDEPILOT-NOTES-PAGE-SEPARATOR#-Page-36\
\
}