#!/bin/bash
(( DEBUG ))  && set -x
function nop()
{
    return -1
}

declare -i VERBOSE=${VERBOSE:-0}
typeset -r -A QUEUES=( ['v100']='v100-localqueue'
		       ['a100']='a100-localqueue'
		       ['h100']='h100-localqueue'
		       ['none']='dummy-localqueue')

typeset -r -i CONTEXT=${CONTEXT:-1}
typeset -r DEFAULT_GPU='v100'
typeset -r GPU=${GPU:-"${DEFAULT_GPU}"}
# name must be lower case and no funny characters
typeset -r NAME=${NAME:-"job"}
typeset -r JOB_ID=${JOBID:-$$}
typeset -r JOB=${JOB:-"${NAME}-${GPU}-${JOB_ID}"}
typeset -r JOBS_DIR_NAME="${JOBS_DIR_NAME:-jobs}"
typeset  -r JOBS_DIR="${JOBS_DIR:-$(pwd)/${JOBS_DIR_NAME}}"
typeset -r CONTAINER="${JOB}-container"
typeset -i -r MAX_SEC="${MAX_SEC:-$(( 60 * 15 ))}"
typeset -i -r JOB_DELETE="${JOB_DELETE:-1}"
typeset -i -r WAIT="${WAIT:-1}"
typeset -i -r JOB_WAIT_TMOUT="${JOB_WAIT_TMOUT:-$(( MAX_SEC * 4 ))}"
typeset OUT=/dev/null 
typeset -i GPU_NUMREQ="${GPU_NUMREQ:-1}"
typeset -i GPU_NUMLIM="${GPU_NUMREQ:-1}"
typeset RESOURCES_YAML=""
typeset COMMAND_YAML=""
#typeset IMAGE=${IMAGE:-"image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/minimal-gpu:2025.1"}
typeset IMAGE="${IMAGE:-"image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/csw-run-f25:latest"}"
typeset CONTEXT_DIR="${CONTEXT_DIR:-$(pwd)}"
typeset OUTPUT_DIR="${JOBS_DIR}/${JOB}"
typeset -r GETLIST="${OUTPUT_DIR}/getlist"
typeset -i rc=0 complete_pid=0 failed_pid=0

# default behavior relies on HOSTNAME set to pod name of where we are running (true on openshift RHOAI)
typeset DEVPOD_NAME=${DEVPOD_NAME:-$HOSTNAME}
typeset DEVCONTAINER=$(oc get pods/${DEVPOD_NAME} -o=jsonpath="{.spec.containers[0].name}")



[[ $1 == -h ]] && {
    cat <<EOF
   gun [-h] <command line>
     gun creates and submits a batch job to a GPU batch queue.  The arguments are treated as a bash command line
     that will execute as the batch job.  The behaviour of the job submission can be controlled via several
     environment variables.  Please, feel free to improve this documentation and fix 
     bugs ;-). By default the files and subdirectories of your current working directory form a context for 
     the job.  The context is copied to the container in which the batch job will execute.  Thus the commandline
     can reference files in the directory.  Additinoally, files created in the working directory of commandline
     in the container will be copied back so that you can inspect output of your job (eg. profiles, logs and outputs).
     These files and directories are placed in a directory make in job specific subdirectory of a directory called jobs.
     Eg.

1. The simple usage run a binary that exsits in the current directory on the default GPU type
$ gun ./hello
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
RUNDIR: jobs/job-v100-9215

Note by default gun waits for the job to complete and displays after the standard output and error of the command line.
After that it display the directory where the outputs for the job where copied.

$ ls jobs/job-v100-9215/
job-v100-9215.log

2. Any additional files created in the remote copy of the context directory are copied to the job output directory
$ gun "./hello | tee hello.log"
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
RUNDIR: jobs/job-v100-9477

$ ls jobs/job-v100-9215
job-v100-9215.log
csw-dev-0.test> ls jobs/job-v100-9477/
hello.log  job-v100-9477.log


3. You can set the NAME varible to provide a unique name for the job to help organized your runs.  Eg.

$ NAME=hello-run0 gun "./hello | tee hello.log"
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
RUNDIR: jobs/hello-run0-v100-9753

$ ls jobs/hello-run0-v100-9753/
hello.log  hello-run-v100-9753.log


4. You can set VERBOSE=1 to see details of what gun is doing (for even more details set DEBUG=1)

$ VERBOSE=1 gun "./hello | tee hello.log"
GPU:v100 JOB:job-v100-10006 CONTAINER:job-v100-10006-container QUEUE:v100-localqueue MAX_SEC:900 WAIT:1 JOB_WAIT: JOB_DELETE:1 CONTEXT_DIR:/opt/app-root/src/cudasudawuda/content/src OUTPUT_DIR=/opt/app-root/src/cudasudawuda/content/src/jobs/job-v100-10006
CMD: ./hello | tee hello.log
apiVersion: batch/v1
kind: Job
metadata:
  name: job-v100-10006
  labels:
    kueue.x-k8s.io/queue-name: v100-localqueue
    test_name: kueue_test
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      maximumExecutionTimeSeconds: 900
      containers:
        - name: job-v100-10006-container
          image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/csw-run-f25:latest
          command: ["/bin/sh", 
                    "-c", 
                    "export RSYNC_RSH='oc rsh -c csw-dev';
                     mkdir job-v100-10006 &&
                     rsync  --archive --no-owner --no-group --omit-dir-times --numeric-ids csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/jobs/job-v100-10006/getlist job-v100-10006/getlist >/dev/null 2>&1 &&
                     rsync   -r --archive --no-owner --no-group --omit-dir-times --numeric-ids --files-from=job-v100-10006/getlist csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/ job-v100-10006/ &&
                     find job-v100-10006 -mindepth 1 -maxdepth 1 > job-v100-10006/gotlist &&
                     cd job-v100-10006 && ./hello | tee hello.log |& tee job-v100-10006.log; cd ..;
                     rsync  --archive --no-owner --no-group --omit-dir-times --no-relative --numeric-ids --exclude-from=job-v100-10006/gotlist job-v100-10006 csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/jobs"]
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
job.batch/job-v100-10006 created
Job Created: job-v100-10006
Name:             job-v100-10006
Namespace:        ja-ope-test
Selector:         batch.kubernetes.io/controller-uid=0894f77d-05a2-4cc0-bc9d-f875514fe2df
Labels:           kueue.x-k8s.io/queue-name=v100-localqueue
                  test_name=kueue_test
Annotations:      <none>
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Suspend:          false
Backoff Limit:    0
Start Time:       Thu, 14 Aug 2025 04:48:39 +0000
Pods Statuses:    1 Active (0 Ready) / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=0894f77d-05a2-4cc0-bc9d-f875514fe2df
           batch.kubernetes.io/job-name=job-v100-10006
           controller-uid=0894f77d-05a2-4cc0-bc9d-f875514fe2df
           job-name=job-v100-10006
           kueue.x-k8s.io/podset=main
  Containers:
   job-v100-10006-container:
    Image:      image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/csw-run-f25:latest
    Port:       <none>
    Host Port:  <none>
    Command:
      /bin/sh
      -c
      export RSYNC_RSH='oc rsh -c csw-dev'; mkdir job-v100-10006 && rsync  --archive --no-owner --no-group --omit-dir-times --numeric-ids csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/jobs/job-v100-10006/getlist job-v100-10006/getlist >/dev/null 2>&1 && rsync   -r --archive --no-owner --no-group --omit-dir-times --numeric-ids --files-from=job-v100-10006/getlist csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/ job-v100-10006/ && find job-v100-10006 -mindepth 1 -maxdepth 1 > job-v100-10006/gotlist && cd job-v100-10006 && ./hello | tee hello.log |& tee job-v100-10006.log; cd ..; rsync  --archive --no-owner --no-group --omit-dir-times --no-relative --numeric-ids --exclude-from=job-v100-10006/gotlist job-v100-10006 csw-dev-0:/opt/app-root/src/cudasudawuda/content/src/jobs
    Limits:
      nvidia.com/gpu:  1
    Requests:
      nvidia.com/gpu:  1
    Environment:       <none>
    Mounts:            <none>
  Volumes:             <none>
  Node-Selectors:      <none>
  Tolerations:         nvidia.com/gpu.product=Tesla-V100-PCIE-32GB:NoSchedule
Events:
  Type    Reason            Age   From                        Message
  ----    ------            ----  ----                        -------
  Normal  Suspended         0s    job-controller              Job suspended
  Normal  CreatedWorkload   0s    batch/job-kueue-controller  Created Workload: ja-ope-test/job-job-v100-10006-285b0
  Normal  Started           0s    batch/job-kueue-controller  Admitted by clusterQueue v100-clusterqueue
  Normal  SuccessfulCreate  0s    job-controller              Created pod: job-v100-10006-88lpd
  Normal  Resumed           0s    job-controller              Job resumed
Waiting job-v100-10006 for condition: complete or failed
job.batch/job-v100-10006 condition met
job-v100-10006: Completed
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
job.batch "job-v100-10006" deleted
RUNDIR: jobs/job-v100-10006


5. If you don't want to wait for the job to complete set WAIT=0.  Note when WAIT=1 the job will be deleted
   automatically for you by gun.  If you set WAIT=0 you will need manage the job.  Eg monitor it and once
   you know it has finished delete.  See 'gobs' (list the status of your undeleted gpu jobs), 
   'gq' (inspect cluster wide gpu queues -- show their status),  'gs' (list all running gpu  pods on cluster nodes), 
   and 'gel' (delete a gpu job)

$ WAIT=0 NAME=myasync-run0 gun "{ nvidia-smi; ./hello; sleep 15; }" 
WARNING: Not waiting for myasync-run0-v100-14109 to finish you must manually delete with 'gobdel myasync-run0-v100-14109'
myasync-run0-v100-14109 jobs/myasync-run0-v100-14109

$ gobs
NAME                      STATUS    COMPLETIONS   DURATION   AGE
myasync-run0-v100-14109   Running   0/1           4s         4s

$ gq
a100-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
dummy-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
h100-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
v100-clusterqueue	 admitted: 1 pending: 0 reserved: 1 resources:3 BestEffortFIFO

$ gs
wrk-3: BUSY 1 ja-ope-test/myasync-run0-v100-14109-67k8c

$ gobs
NAME                      STATUS    COMPLETIONS   DURATION   AGE
myasync-run0-v100-14109   Running   0/1           23s        23s

$ gq
a100-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
dummy-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
h100-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:0 BestEffortFIFO
v100-clusterqueue	 admitted: 0 pending: 0 reserved: 0 resources:3 BestEffortFIFO

$ gobs
NAME                      STATUS     COMPLETIONS   DURATION   AGE
myasync-run0-v100-14109   Complete   1/1           26s        32s

$ gs

$ cat jobs/myasync-run0-v100-14109/myasync-run0-v100-14109.log 
Thu Aug 14 05:36:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0             25W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU

$ gel myasync-run0-v100-14109
job.batch "myasync-run0-v100-14109" deleted


6. Note the tool chain is available in the batch pod

$  gun "make hello && ./hello | tee hello.log"
nvcc  hello.cu -o hello
Hello from CPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
Hello from GPU
RUNDIR: jobs/job-v100-10422
$
 

See the top of this message for simple overview and the source for the details on how you can use gun ;-)
EOF
    exit 0
}

(( VERBOSE )) && OUT=/dev/stderr

[[ ! -v QUEUES[$GPU] ]] && {
    echo "ERROR: unsupported GPU $GPU : no queue found" > /dev/stderr
    exit -1
}
typeset QUEUE=${QUEUES[$GPU]}

if [[ "$GPU" == 'none' ]]; then
    RESOURCES_YAML=\
"          resources:
            requests:
              cpu: \"1\"
              memory: \"1Gi\"
            limits:
              cpu: \"1\"
              memory: \"1Gi\"
"
else
    RESOURCES_YAML=\
"          resources:
            requests:
              nvidia.com/gpu: ${GPU_NUMREQ}
            limits:
              nvidia.com/gpu: ${GPU_NUMLIM}
"
fi

cmd="$@"

[[ -z "$cmd" ]] && {
    cat > /dev/stderr <<EOF
USAGE: ${0##*/} <command line>
  Runs the specified command on a GPU node
  Supported GPU types are:  ${!QUEUES[@]}
  Default GPU type is $DEFAULT_GPU
  To overide default set GPU enviroment variable eg.
     $ GPU=a100 ${0##*/} nvidia-smi
EOF
    exit 0
}

(( VERBOSE )) && {
    echo "GPU:${GPU} JOB:${JOB} CONTAINER:${CONTAINER} QUEUE:$QUEUE" \
	 "MAX_SEC:${MAX_SEC} WAIT:${WAIT} JOB_WAIT:${JOB_WAIT} JOB_DELETE:${JOB_DELETE}" \
	 "CONTEXT_DIR:${CONTEXT_DIR} OUTPUT_DIR=${OUTPUT_DIR}" > /dev/stderr
    echo "CMD: $cmd" > /dev/stderr
}

if (( CONTEXT )); then
    [[ ! -d $CONTEXT_DIR ]] && {
	echo "ERROR: CONTEXT_DIR: $CONTEXT_DIR is not a directory"
	exit -1
    }
    [[ -a ${OUTPUT_DIR} ]] && {
	echo "ERROR: $OUTPUT_DIR directory already exits"
	exit -1
    }

    ! mkdir -p ${OUTPUT_DIR}  && {
	echo "ERROR: Failed to make output dir"
	exit -1
    }
    
    # create list of context to send files subdirectories for
    # of $CONTEXT_DIR excluding the ${JOBS_DIR} if a direct
    # child of $CONTEXT_DIR
    (
	jdir="${JOBS_DIR#${CONTEXT_DIR}/}"
	[[ "$jdir" != "$JOBS_DIR" ]] && {
	    jdir="./${jdir}"
	}
	cd ${CONTEXT_DIR}
	find . -mindepth 1 -maxdepth 1 -not -path ${jdir} -prune  > ${GETLIST}
    )
    (( ! VERBOSE )) && {
	RSYNC_VERBOSE="-q"
    }
    COMMAND_YAML=$(cat <<EOF
          command: ["/bin/sh", 
                    "-c", 
                    "export RSYNC_RSH='oc rsh -c ${DEVCONTAINER}';
                     mkdir ${JOB} &&
                     rsync ${RSYNC_VERBOSE} --archive --no-owner --no-group --omit-dir-times --numeric-ids ${DEVPOD_NAME}:${GETLIST} ${JOB}/getlist >/dev/null 2>&1 &&
                     rsync ${RSYNC_VERBOSE}  -r --archive --no-owner --no-group --omit-dir-times --numeric-ids --files-from=${JOB}/getlist ${DEVPOD_NAME}:${CONTEXT_DIR}/ ${JOB}/ &&
                     find ${JOB} -mindepth 1 -maxdepth 1 > ${JOB}/gotlist &&
                     cd ${JOB} && ${cmd} |& tee $JOB.log; cd ..;
                     rsync ${RSYNC_VERBOSE} --archive --no-owner --no-group --omit-dir-times --no-relative --numeric-ids --exclude-from=${JOB}/gotlist ${JOB} ${DEVPOD_NAME}:${JOBS_DIR}"]
EOF
		   )
else
    COMMAND_YAML=$(cat <<EOF 
          command: ["/bin/sh", "-c", "${cmd}"]
EOF
		)
fi

YAML=$(cat <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: ${JOB}
  labels:
    kueue.x-k8s.io/queue-name: ${QUEUE}
    test_name: kueue_test
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      maximumExecutionTimeSeconds: ${MAX_SEC}
      containers:
        - name: ${CONTAINER}
          image: ${IMAGE}
${COMMAND_YAML}
${RESOURCES_YAML}
EOF
)

(( VERBOSE )) && echo "$YAML"

oc apply -f - >$OUT 2>&1 <<< "$YAML"

(( VERBOSE )) && {
    echo "Job Created: $JOB" > $OUT
    oc describe job $JOB > $OUT 2>&1
}


if (( WAIT )); then
    trap nop SIGINT

    oc wait --for=condition=complete --timeout=${JOB_WAIT_TMOUT}s jobs/$JOB >$OUT 2>&1 & complete_pid=$!   
    oc wait --for=condition=failed --timeout=${JOB_WAIT_TMOUT}s jobs/$JOB >$OUT 2>&1 && exit -1 & failed_pid=$!

    echo "Waiting $JOB for condition: complete or failed" > $OUT
    wait -n $complete_pid $failed_pid
    rc=$?
    
    if ((  $rc == 0 )); then
	echo "$JOB: Completed" >$OUT
    else
	echo "*** ERROR: $JOB FAILED!!!! *****"
    fi
    
    oc logs jobs/$JOB
    
    (( JOB_DELETE )) && { 
	oc delete jobs/$JOB >$OUT 2>&1
    }
    
    (( CONTEXT )) && {
	[[ -a ${OUTPUT_DIR}/getlist ]] && rm ${OUTPUT_DIR}/getlist
	echo RUNDIR: ${OUTPUT_DIR#$(pwd)/} > /dev/stderr
    }
else
    echo "WARNING: Not waiting for $JOB to finish you must manually delete with 'gobdel $JOB'" > /dev/stderr
    if (( CONTEXT )); then 
	echo $JOB ${OUTPUT_DIR#$(pwd)/}
    else
	echo $JOB
    fi
fi
